{
 "metadata": {
  "name": "",
  "signature": "sha256:ca324780b8799fe1c04a619fe94d072ff246860a2e5fb9a7a793896bcc2175bb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Nengo Example: Learning to compute a product\n",
      "\n",
      "Unlike the communication channel and the element-wise square,\n",
      "the product is a nonlinear function on multiple inputs.\n",
      "This represents a difficult case for learning rules\n",
      "that aim to generalize a function given many\n",
      "input-output example pairs.\n",
      "However, using the same type of network structure\n",
      "as in the communication channel and square cases,\n",
      "we can learn to compute the product of two dimensions\n",
      "with the `nengo.PES` learning rule."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "import nengo\n",
      "from nengo.processes import WhiteNoise"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Step 1: Create the model\n",
      "\n",
      "Like previous examples, the network consists of `pre`, `post`, and `error` ensembles.\n",
      "We'll use two-dimensional white noise input and attempt to learn the product\n",
      "using the actual product to compute the error signal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = nengo.Network()\n",
      "with model:\n",
      "    # -- input and pre popluation\n",
      "    inp = nengo.Node(WhiteNoise(60, high=5).f(d=2))\n",
      "    pre = nengo.Ensemble(120, dimensions=2)\n",
      "    nengo.Connection(inp, pre)\n",
      "    \n",
      "    # -- error population\n",
      "    prod_node = nengo.Node(lambda t, x: x[0] * x[1], size_in=2)  # We'll give it the actual product\n",
      "    nengo.Connection(inp, prod_node, synapse=None)\n",
      "    error = nengo.Ensemble(60, dimensions=1)\n",
      "    nengo.Connection(prod_node, error)\n",
      "    \n",
      "    # -- inhibit error after 40 seconds\n",
      "    inhib = nengo.Node(lambda t: 2.0 if t > 40.0 else 0.0)\n",
      "    nengo.Connection(inhib, error.neurons, transform=[[-1]] * error.n_neurons)\n",
      "\n",
      "    # -- post population\n",
      "    post = nengo.Ensemble(60, dimensions=1)\n",
      "    nengo.Connection(post, error, transform=-1)\n",
      "    error_conn = nengo.Connection(error, post, modulatory=True)\n",
      "    nengo.Connection(pre, post,\n",
      "                     function=lambda x: np.random.random(1),\n",
      "                     learning_rule_type=nengo.PES(error_conn))\n",
      "    \n",
      "    # -- probes\n",
      "    prod_p = nengo.Probe(prod_node)\n",
      "    pre_p = nengo.Probe(pre, synapse=0.01)\n",
      "    post_p = nengo.Probe(post, synapse=0.01)\n",
      "    error_p = nengo.Probe(error, synapse=0.03)\n",
      "\n",
      "sim = nengo.Simulator(model)\n",
      "sim.run(60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 8))\n",
      "plt.subplot(3, 1, 1)\n",
      "plt.plot(sim.trange(), sim.data[pre_p], c='b')\n",
      "plt.legend(('Pre decoding',), loc='best')\n",
      "plt.subplot(3, 1, 2)\n",
      "plt.plot(sim.trange(), sim.data[prod_p], c='k', label='Actual product')\n",
      "plt.plot(sim.trange(), sim.data[post_p], c='r', label='Post decoding')\n",
      "plt.legend(loc='best')\n",
      "plt.subplot(3, 1, 3)\n",
      "plt.plot(sim.trange(), sim.data[error_p], c='b')\n",
      "plt.ylim(-1, 1)\n",
      "plt.legend((\"Error\",), loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's zoom in on the network at the beginning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 8))\n",
      "plt.subplot(3, 1, 1)\n",
      "plt.plot(sim.trange()[:2000], sim.data[pre_p][:2000], c='b')\n",
      "plt.legend(('Pre decoding',), loc='best')\n",
      "plt.subplot(3, 1, 2)\n",
      "plt.plot(sim.trange()[:2000], sim.data[prod_p][:2000], c='k', label='Actual product')\n",
      "plt.plot(sim.trange()[:2000], sim.data[post_p][:2000], c='r', label='Post decoding')\n",
      "plt.legend(loc='best')\n",
      "plt.subplot(3, 1, 3)\n",
      "plt.plot(sim.trange()[:2000], sim.data[error_p][:2000], c='b')\n",
      "plt.ylim(-1, 1)\n",
      "plt.legend((\"Error\",), loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now right where we turn off learning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 8))\n",
      "plt.subplot(3, 1, 1)\n",
      "plt.plot(sim.trange()[38000:42000], sim.data[pre_p][38000:42000], c='b')\n",
      "plt.legend(('Pre decoding',), loc='best')\n",
      "plt.subplot(3, 1, 2)\n",
      "plt.plot(sim.trange()[38000:42000], sim.data[prod_p][38000:42000], c='k', label='Actual product')\n",
      "plt.plot(sim.trange()[38000:42000], sim.data[post_p][38000:42000], c='r', label='Post decoding')\n",
      "plt.legend(loc='best')\n",
      "plt.subplot(3, 1, 3)\n",
      "plt.plot(sim.trange()[38000:42000], sim.data[error_p][38000:42000], c='b')\n",
      "plt.ylim(-1, 1)\n",
      "plt.legend((\"Error\",), loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that it has learned a decent approximation of the product,\n",
      "but it's not perfect -- typically, it's not as good as the offline optimization.\n",
      "The reason for this is that we've given it white noise input,\n",
      "which has a mean of 0; since this happens in both dimensions,\n",
      "we'll see a lot of examples of inputs and outputs near 0.\n",
      "In other words, we've oversampled a certain part of the\n",
      "vector space, and overlearned decoders that do well in\n",
      "that part of the space. If we want to do better in other\n",
      "parts of the space, we would need to construct an input\n",
      "signal that evenly samples the space."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
